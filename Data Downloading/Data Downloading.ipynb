{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44090665-fd88-4660-a973-c446ea3c1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLDAS data txt file procedure ##\n",
    "# Go to this GLDAS data download link (https://disc.gsfc.nasa.gov/datasets?keywords=GLDAS) \n",
    "#select the subset/get data which you want to downlaod.\n",
    "#Then click on Get data\n",
    "#Then it will give the txt file with monthly GLDAS data links \n",
    "#Save the txt file and keep the txt file path in the below code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68edbb6d-874e-4b37-82ca-ee2cde4e2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Earthdata login token ##\n",
    "# Open the Earthdata login page (https://urs.earthdata.nasa.gov/)\n",
    "# Login with your earth data login credintials\n",
    "# Click on the Generate token\n",
    "# Click on the show token\n",
    "# Copy the Earthdata token and replace with token in the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6f143-2948-4be3-adf3-1708e754c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # For downloading files from URLs\n",
    "import os  # For managing file paths and directories\n",
    "import xarray as xr  # For handling NetCDF files\n",
    "\n",
    "# Function to download a file from a given URL\n",
    "def download_file(url, directory, token):\n",
    "    filename = url.split('/')[-1]  # Extract the file name from the URL\n",
    "    filepath = os.path.join(directory, filename)  # Create the full path for saving the file\n",
    "    with requests.get(url, stream=True, headers={'Authorization': f'Bearer {token}'}) as r:  # Send a GET request with token\n",
    "        r.raise_for_status()  # Ensure the request was successful\n",
    "        with open(filepath, 'wb') as f:  # Open the file in binary write mode\n",
    "            for chunk in r.iter_content(chunk_size=8192):  # Download the file in chunks\n",
    "                f.write(chunk)  # Write each chunk to the file\n",
    "    print(f\"Downloaded {filename}\")  # Notify completion\n",
    "    return filepath  # Return the path to the saved file\n",
    "\n",
    "# Function to read URLs from a text file\n",
    "def read_urls_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:  # Open the file containing URLs\n",
    "        urls = file.readlines()  # Read all lines (each URL is a line)\n",
    "    urls = [url.strip() for url in urls]  # Remove newline characters\n",
    "    return urls  # Return a clean list of URLs\n",
    "\n",
    "# Set the directory to save downloaded files\n",
    "download_directory = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")  # Save files to Desktop\n",
    "os.makedirs(download_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Earthdata login token (replace 'your_token_here' with your Earthdata token)\n",
    "token = 'eyJ0eXAiOiJKV1QiLCJvcmlnaW4iOiJFYXJ0aGRhdGEgTG9naW4iLCJzaWciOiJlZGxqd3RwdWJrZXlfb3BzIiwiYWxnIjoiUlMyNTYifQ.eyJ0eXBlIjoiVXNlciIsInVpZCI6InJvbmlraWFuamkiLCJleHAiOjE3MjI5NDc1MDAsImlhdCI6MTcxNzc2MzUwMCwiaXNzIjoiRWFydGhkYXRhIExvZ2luIn0.13p6Ci3ABBYACNP8pLvTamO0bMwAhJGP3xl88O1DD9vOu1V70ti_XZ0iBGZ6yZ87QWcp-16uPHqSFqGheG19zdVOxDSEIFniUaOVvG4mF-2pxMuy6KDsB0AFC-hLwKAFgMERibK05UCAKgrMj3P1Sf_BITjDSMvuo60cTnzG8TeWkGC5NLr53cQ6v1hKfbIyJP86K-s4g-PSBaEMWnqU7qeXPwbxaVvJ1EkHlkiQ3Xnxih8OVrR3D6OxEUBADilxr3sxK4V9ohsaCQxn-7tWQUuGOgn-yh13eKK2erVWLgpMaegv0QxLgp5aEz7qUWzUs0SnJ6uBKEB3VEsK7ABryg'\t\n",
    "\n",
    "# Path to the text file containing URLs (replace 'your file path.txt' with your original txt file path which contains GLDAS data links)\n",
    "urls_file_path = r\"your file path.txt\"  # Replace with your txt file path\n",
    "\n",
    "# Read the list of URLs\n",
    "urls = read_urls_from_file(urls_file_path)  # Extract URLs from the text file\n",
    "\n",
    "# Download each file and store the paths\n",
    "filepaths = []  # List to store paths of downloaded files\n",
    "for url in urls:  # Iterate through each URL\n",
    "    filepaths.append(download_file(url, download_directory, token))  # Download the file and append its path\n",
    "\n",
    "# Merge downloaded NetCDF files\n",
    "merged_ds = xr.open_mfdataset(filepaths, combine='by_coords', engine='netcdf4')  # Merge files by shared coordinates\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_filepath = os.path.join(download_directory, \"your_merged_file.nc\")  # (replace 'your_merged_all' with your file path)\n",
    "merged_ds.to_netcdf(merged_filepath)  # Save the merged dataset\n",
    "print(\"your_merged_file saved successfully:\", merged_filepath)  # Notify completion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
